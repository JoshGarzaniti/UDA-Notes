{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc6899-32f6-4f5e-873f-f463341faf8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Unstructured Data Analytics\\text_prep.qmd:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mregex\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m PorterStemmer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "love_strings = [\"love\", \"loving\", \"loved\", \"lover\"]\n",
    "\n",
    "[stemmer.stem(word) for word in love_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff69c73-6c73-48d7-9e5b-c38216f0848c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Unstructured Data Analytics\\text_prep.qmd:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mregex\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m PorterStemmer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "love_strings = [\"love\", \"loving\", \"loved\", \"lover\"]\n",
    "\n",
    "[stemmer.stem(word) for word in love_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2dcd57-af16-46e8-b83b-33262e9d0672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'love', 'love', 'lover']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "love_strings = [\"love\", \"loving\", \"loved\", \"lover\"]\n",
    "\n",
    "[stemmer.stem(word) for word in love_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9c91f-237f-41c4-ada0-29a2ac45bc14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Unstructured Data Analytics\\text_prep.qmd:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# pip3 import scikit-learn\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      4\u001b[0m documents \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mve loved you three summers now, honey, but I want them all\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      5\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mYou\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre my, my, my, my lover\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      6\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mTell me, tell me if you love me or not\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      7\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mTryna do what lovers do, ooh\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      8\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mI just, I can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt, I just can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be loving you no more\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# pip3 import scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\"I've loved you three summers now, honey, but I want them all\", \n",
    "              \"You're my, my, my, my lover\", \n",
    "              \"Tell me, tell me if you love me or not\", \n",
    "              \"Tryna do what lovers do, ooh\", \n",
    "              \"I just, I can't, I just can't be loving you no more\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "tfidf_matrix.toarray()\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68b221-2b24-4add-ad6d-41fe0c2b0726",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Unstructured Data Analytics\\text_prep.qmd:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# pip3 import scikit-learn\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      4\u001b[0m documents \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mve loved you three summers now, honey, but I want them all\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      5\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mYou\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre my, my, my, my lover\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      6\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mTell me, tell me if you love me or not\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      7\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mTryna do what lovers do, ooh\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      8\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mI just, I can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt, I just can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be loving you no more\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# pip3 import scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\"I've loved you three summers now, honey, but I want them all\", \n",
    "              \"You're my, my, my, my lover\", \n",
    "              \"Tell me, tell me if you love me or not\", \n",
    "              \"Tryna do what lovers do, ooh\", \n",
    "              \"I just, I can't, I just can't be loving you no more\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "tfidf_matrix.toarray()\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a26c8-3968-46fc-bf76-01756abf81cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['all', 'be', 'but', 'can', 'do', 'honey', 'if', 'just', 'love',\n",
       "       'loved', 'lover', 'lovers', 'loving', 'me', 'more', 'my', 'no',\n",
       "       'not', 'now', 'ooh', 'or', 're', 'summers', 'tell', 'them',\n",
       "       'three', 'tryna', 've', 'want', 'what', 'you'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip3 import scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\"I've loved you three summers now, honey, but I want them all\", \n",
    "              \"You're my, my, my, my lover\", \n",
    "              \"Tell me, tell me if you love me or not\", \n",
    "              \"Tryna do what lovers do, ooh\", \n",
    "              \"I just, I can't, I just can't be loving you no more\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "tfidf_matrix.toarray()\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff4a56-0c63-4bee-903a-eb4e79b9ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_docs = [stemmer.stem(word) for word in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4700f28-a02a-456c-a416-e7eb29e32106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['al', 'be', 'but', 'can', 'do', 'honey', 'if', 'just', 'lov',\n",
       "       'love', 'loved', 'lovers', 'loving', 'me', 'mor', 'my', 'no',\n",
       "       'not', 'now', 'ooh', 'or', 're', 'summers', 'tell', 'them',\n",
       "       'three', 'tryna', 've', 'want', 'what', 'you'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix_stemmed = vectorizer.fit_transform(stemmed_docs)\n",
    "\n",
    "tfidf_matrix_stemmed.toarray()\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf3d03-7411-4904-8fa6-8c3c245bffc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Unstructured Data Analytics\\text_prep.qmd:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m  \u001b[39m# python -m spacy download en_core_web_lg\u001b[39;00m\n\u001b[0;32m      4\u001b[0m love_strings \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mlove\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mloving\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mloved\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlover\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    " # python -m spacy download en_core_web_lg\n",
    "\n",
    "love_strings = [\"love\", \"loving\", \"loved\", \"lover\"]\n",
    "\n",
    "[stemmer.stem(word) for word in love_strings]\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "docs = list(nlp.pipe(love_strings))\n",
    "\n",
    "lemmatized_tokens = [docs[x][0].lemma_ for x in range(len(docs))]\n",
    "\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b400231-40de-4ddf-afd2-aaeb500784c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Unstructured Data Analytics\\text_prep.qmd:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m  \u001b[39m# python -m spacy download en_core_web_lg\u001b[39;00m\n\u001b[0;32m      4\u001b[0m love_strings \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mlove\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mloving\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mloved\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlover\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    " # python -m spacy download en_core_web_lg\n",
    "\n",
    "love_strings = [\"love\", \"loving\", \"loved\", \"lover\"]\n",
    "\n",
    "[stemmer.stem(word) for word in love_strings]\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "docs = list(nlp.pipe(love_strings))\n",
    "\n",
    "lemmatized_tokens = [docs[x][0].lemma_ for x in range(len(docs))]\n",
    "\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116c80a-3e4a-4d0b-9cd7-50d8938a4966",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-a8f686fd0209>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m spacy download en_core_web_lg\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy download en_core_web_lg\n",
    "import spacy\n",
    " # python -m spacy download en_core_web_lg\n",
    "\n",
    "love_strings = [\"love\", \"loving\", \"loved\", \"lover\"]\n",
    "\n",
    "[stemmer.stem(word) for word in love_strings]\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "docs = list(nlp.pipe(love_strings))\n",
    "\n",
    "lemmatized_tokens = [docs[x][0].lemma_ for x in range(len(docs))]\n",
    "\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m PorterStemmer\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mspacy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mspacy\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#import spacy #Having trouble with installing spacy in the console\n",
    " # python -m spacy download en_core_web_lg\n",
    "import pandas as pd\n",
    "import pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lotrc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lotrc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lotrc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Last_Statements = pd.read_csv(r\"C:\\Unstructured Data Analytics\\last_statements.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          statements\n",
      "0  Yes ma'am, first I would like to apologize for...\n",
      "1  Yes Warden, I would like to thank everyone, al...\n",
      "2  Yes. I want to say thank you to all the people...\n",
      "3  Yes  ma'am, to the Townsend Family, I'm sorry ...\n",
      "4  I'd like to address the Kitchens and Mosqueda ...\n"
     ]
    }
   ],
   "source": [
    "print(Last_Statements.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lematizer = nltk.WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statements = Last_Statements['statements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Yes ma'am, first I would like to apologize for...\n",
      "1      Yes Warden, I would like to thank everyone, al...\n",
      "2      Yes. I want to say thank you to all the people...\n",
      "3      Yes  ma'am, to the Townsend Family, I'm sorry ...\n",
      "4      I'd like to address the Kitchens and Mosqueda ...\n",
      "                             ...                        \n",
      "586    I pray that my family will rejoice and will fo...\n",
      "587     When asked if he had a last statement, he rep...\n",
      "588    What is about to transpire in a few moments is...\n",
      "589       This inmate declined to make a last statement.\n",
      "590    Statement to the Media: I, at this very moment...\n",
      "Name: statements, Length: 591, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ')' does not match opening parenthesis '[' (<ipython-input-10-c7f5d20559a8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Stemmed_Statements = \" \".join([stemmer.stem(word) for word in Sentences)]\u001b[0m\n\u001b[1;37m                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m closing parenthesis ')' does not match opening parenthesis '['\n"
     ]
    }
   ],
   "source": [
    "Stemmed_Statements = \" \".join([stemmer.stem(word) for word in Sentences)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Stemmed_Statements \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m Sentences])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sentences' is not defined"
     ]
    }
   ],
   "source": [
    "Stemmed_Statements = \" \".join([stemmer.stem(word) for word in Sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lotrc/nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lotrc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Sentences \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(Statements)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lotrc/nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lotrc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "Sentences = nltk.word_tokenize(Statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Stemmed_Statements \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m Sentences])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sentences' is not defined"
     ]
    }
   ],
   "source": [
    "Stemmed_Statements = \" \".join([stemmer.stem(word) for word in Sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Stemmed_Statements \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39mstem(Sentences) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m Sentences])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sentences' is not defined"
     ]
    }
   ],
   "source": [
    "Stemmed_Statements = \" \".join([stemmer.stem(Sentences) for word in Sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lotrc/nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lotrc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Stemmed_Statements \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39mstem(word) \n\u001b[1;32m----> 2\u001b[0m                                 \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39;49mword_tokenize(statement)]) \n\u001b[0;32m      3\u001b[0m                                 \u001b[39mfor\u001b[39;00m statement \u001b[39min\u001b[39;00m Statements]\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lotrc/nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lotrc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "Stemmed_Statements = [\" \".join([stemmer.stem(word) \n",
    "                                for word in nltk.word_tokenize(statement)]) \n",
    "                                for statement in Statements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Stemmed_Statements' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStemmed Sentence:\u001b[39m\u001b[39m\"\u001b[39m, Stemmed_Statements)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Stemmed_Statements' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Sentence:\", Stemmed_Statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lotrc/nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lotrc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Lemmatized_Statements \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lemmatizer\u001b[39m.\u001b[39mlemmatize(word) \n\u001b[1;32m----> 2\u001b[0m                                    \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39;49mword_tokenize(statement)]) \n\u001b[0;32m      3\u001b[0m                                    \u001b[39mfor\u001b[39;00m statement \u001b[39min\u001b[39;00m Statements]\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lotrc/nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lotrc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "Lemmatized_Statements = [\" \".join([lemmatizer.lemmatize(word) \n",
    "                                   for word in nltk.word_tokenize(statement)]) \n",
    "                                   for statement in Statements]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tfidf_matrix \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(Statements)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1264\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:99\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m decoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     doc \u001b[39m=\u001b[39m decoder(doc)\n\u001b[0;32m    100\u001b[0m \u001b[39mif\u001b[39;00m analyzer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m     doc \u001b[39m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:232\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    229\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_error)\n\u001b[0;32m    231\u001b[0m \u001b[39mif\u001b[39;00m doc \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mnan:\n\u001b[1;32m--> 232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(Statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           statements\n",
      "0   Yes ma'am, first I would like to apologize for...\n",
      "1   Yes Warden, I would like to thank everyone, al...\n",
      "2   Yes. I want to say thank you to all the people...\n",
      "3   Yes  ma'am, to the Townsend Family, I'm sorry ...\n",
      "4   I'd like to address the Kitchens and Mosqueda ...\n",
      "5   Yes. I would Warden I call upon peace. To the ...\n",
      "6    Yes Warden, I would like to tell the family o...\n",
      "7   Yes Warden, To the family of the victim I want...\n",
      "8   What is occurring  here tonight is not justice...\n",
      "9   Vetta, Jared, Ray I’m  sorry, no I’m not sorry...\n",
      "10  Yes ma’am, I want to  thank y’all. I love y’al...\n",
      "11  I would like to  apologize to the Nix family f...\n",
      "12                           No last statement given.\n",
      "13  I want to take this moment to be shared with e...\n",
      "14  Yes, I just want to  thank (pause) I don’t wan...\n",
      "15  I just want to say to  the family of Pablo Cas...\n",
      "16  I would like to thank my  Jesus Christ my Lord...\n",
      "17  Ok to start with I would like to thank everyon...\n",
      "18                             No statement was made.\n",
      "19  Yes, when they lay me  down to sleep, for I am...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Last_Statements.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-93f614f0dc87>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    sum(is.na(Last_Statements))\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sum(is.na(Last_Statements))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statements    2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_count = Last_Statements.isna().sum()\n",
    "print(nan_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Last_Statements = Last_Statements.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Yes ma'am, first I would like to apologize for...\n",
      "1      Yes Warden, I would like to thank everyone, al...\n",
      "2      Yes. I want to say thank you to all the people...\n",
      "3      Yes  ma'am, to the Townsend Family, I'm sorry ...\n",
      "4      I'd like to address the Kitchens and Mosqueda ...\n",
      "                             ...                        \n",
      "586    I pray that my family will rejoice and will fo...\n",
      "587     When asked if he had a last statement, he rep...\n",
      "588    What is about to transpire in a few moments is...\n",
      "589       This inmate declined to make a last statement.\n",
      "590    Statement to the Media: I, at this very moment...\n",
      "Name: statements, Length: 589, dtype: object\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lotrc/nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lotrc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mlen\u001b[39m(Statements)\n\u001b[0;32m      9\u001b[0m \u001b[39m#Stemming example\u001b[39;00m\n\u001b[0;32m     10\u001b[0m Stemmed_Statements \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([stemmer\u001b[39m.\u001b[39mstem(word) \n\u001b[1;32m---> 11\u001b[0m                                 \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39;49mword_tokenize(statement)]) \n\u001b[0;32m     12\u001b[0m                                 \u001b[39mfor\u001b[39;00m statement \u001b[39min\u001b[39;00m Statements]\n\u001b[0;32m     14\u001b[0m \u001b[39m#Lematization example\u001b[39;00m\n\u001b[0;32m     15\u001b[0m Lemmatized_Statements \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lemmatizer\u001b[39m.\u001b[39mlemmatize(word) \n\u001b[0;32m     16\u001b[0m                                    \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m nltk\u001b[39m.\u001b[39mword_tokenize(statement)]) \n\u001b[0;32m     17\u001b[0m                                    \u001b[39mfor\u001b[39;00m statement \u001b[39min\u001b[39;00m Statements]\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\lotrc\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\lotrc/nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\lotrc\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\lotrc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lematizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "Statements = Last_Statements['statements']\n",
    "\n",
    "print(Statements)\n",
    "\n",
    "len(Statements)\n",
    "#Stemming example\n",
    "Stemmed_Statements = [\" \".join([stemmer.stem(word) \n",
    "                                for word in nltk.word_tokenize(statement)]) \n",
    "                                for statement in Statements]\n",
    "\n",
    "#Lematization example\n",
    "Lemmatized_Statements = [\" \".join([lemmatizer.lemmatize(word) \n",
    "                                   for word in nltk.word_tokenize(statement)]) \n",
    "                                   for statement in Statements]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01', '06', '09', ..., 'youth', 'zena', 'zero'],\n",
       "      shape=(3647,), dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(Statements)\n",
    "\n",
    "tfidf_matrix.toarray()\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vocabulary\u001b[39m.\u001b[39;49munique()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "vocabulary.unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01',\n",
       " '06',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '10th',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '14th',\n",
       " '15',\n",
       " '16',\n",
       " '18',\n",
       " '180',\n",
       " '19',\n",
       " '1983',\n",
       " '1990',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1998',\n",
       " '1st',\n",
       " '20',\n",
       " '2000',\n",
       " '2003',\n",
       " '2005',\n",
       " '2006',\n",
       " '2008',\n",
       " '21',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '27',\n",
       " '27th',\n",
       " '28',\n",
       " '30',\n",
       " '300',\n",
       " '31',\n",
       " '31b',\n",
       " '34',\n",
       " '35',\n",
       " '36',\n",
       " '39',\n",
       " '53',\n",
       " '5th',\n",
       " '75',\n",
       " '79',\n",
       " '83',\n",
       " '85',\n",
       " '8½',\n",
       " '903',\n",
       " '99',\n",
       " 'aaron',\n",
       " 'abel',\n",
       " 'abide',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abraham',\n",
       " 'abrahams',\n",
       " 'absolutely',\n",
       " 'absurdity',\n",
       " 'abul',\n",
       " 'abused',\n",
       " 'accept',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'ache',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledging',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'actions',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'acts',\n",
       " 'actually',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'add',\n",
       " 'added',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'adela',\n",
       " 'adios',\n",
       " 'administration',\n",
       " 'admit',\n",
       " 'admitted',\n",
       " 'adonya',\n",
       " 'advanced',\n",
       " 'advisor',\n",
       " 'advocacy',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'afflictions',\n",
       " 'afforded',\n",
       " 'affording',\n",
       " 'afraid',\n",
       " 'african',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreeing',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'aid',\n",
       " 'aided',\n",
       " 'aiding',\n",
       " 'ailsa',\n",
       " 'ain',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'alabamos',\n",
       " 'alaikum',\n",
       " 'alan',\n",
       " 'alba',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'aleda',\n",
       " 'ali',\n",
       " 'alicia',\n",
       " 'alike',\n",
       " 'alison',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'alleged',\n",
       " 'allen',\n",
       " 'allison',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'almighty',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'altered',\n",
       " 'although',\n",
       " 'always',\n",
       " 'alyssa',\n",
       " 'am',\n",
       " 'amanda',\n",
       " 'amazing',\n",
       " 'amber',\n",
       " 'ambitions',\n",
       " 'amen',\n",
       " 'amended',\n",
       " 'amenia',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amezcua',\n",
       " 'amigos',\n",
       " 'amount',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'ancestors',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andy',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angelic',\n",
       " 'angelo',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angered',\n",
       " 'angie',\n",
       " 'angry',\n",
       " 'anil',\n",
       " 'animosity',\n",
       " 'anitra',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'anne',\n",
       " 'annette',\n",
       " 'annie',\n",
       " 'anointest',\n",
       " 'anointeth',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'anticipated',\n",
       " 'anticipation',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apologies',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'appeal',\n",
       " 'appeals',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appointed',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'approximately',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'arguing',\n",
       " 'arguments',\n",
       " 'arm',\n",
       " 'armando',\n",
       " 'arms',\n",
       " 'arnold',\n",
       " 'arnott',\n",
       " 'around',\n",
       " 'arrested',\n",
       " 'arresting',\n",
       " 'arrival',\n",
       " 'arrogant',\n",
       " 'art',\n",
       " 'arthur',\n",
       " 'article',\n",
       " 'articulate',\n",
       " 'as',\n",
       " 'asdadu',\n",
       " 'ashamed',\n",
       " 'ashanti',\n",
       " 'ashlee',\n",
       " 'ashley',\n",
       " 'ashworth',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'assalaam',\n",
       " 'assigned',\n",
       " 'assistance',\n",
       " 'assisted',\n",
       " 'association',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'asthma',\n",
       " 'at',\n",
       " 'ate',\n",
       " 'atrocity',\n",
       " 'attacked',\n",
       " 'attend',\n",
       " 'attitude',\n",
       " 'attorney',\n",
       " 'attorneys',\n",
       " 'attributes',\n",
       " 'atw',\n",
       " 'aubrey',\n",
       " 'audio',\n",
       " 'audrey',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'aunts',\n",
       " 'authority',\n",
       " 'autumn',\n",
       " 'available',\n",
       " 'avenge',\n",
       " 'avenged',\n",
       " 'aviation',\n",
       " 'awake',\n",
       " 'awaken',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'ayers',\n",
       " 'ba',\n",
       " 'babcock',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'background',\n",
       " 'bad',\n",
       " 'baggett',\n",
       " 'baisey',\n",
       " 'bakers',\n",
       " 'ball',\n",
       " 'ballistics',\n",
       " 'barbados',\n",
       " 'barbara',\n",
       " 'barbarity',\n",
       " 'barker',\n",
       " 'barry',\n",
       " 'based',\n",
       " 'basis',\n",
       " 'bass',\n",
       " 'bassinet',\n",
       " 'bathroom',\n",
       " 'battle',\n",
       " 'be',\n",
       " 'bea',\n",
       " 'bear',\n",
       " 'bears',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beatings',\n",
       " 'beatitudes',\n",
       " 'beautiful',\n",
       " 'became',\n",
       " 'because',\n",
       " 'becca',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'been',\n",
       " 'beer',\n",
       " 'before',\n",
       " 'beg',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'beginnings',\n",
       " 'begins',\n",
       " 'begotten',\n",
       " 'behalf',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'being',\n",
       " 'beings',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'believes',\n",
       " 'believeth',\n",
       " 'believing',\n",
       " 'bella',\n",
       " 'belong',\n",
       " 'belonged',\n",
       " 'belongs',\n",
       " 'beloved',\n",
       " 'bernard',\n",
       " 'beside',\n",
       " 'best',\n",
       " 'bestow',\n",
       " 'beth',\n",
       " 'bethrie',\n",
       " 'betrayed',\n",
       " 'better',\n",
       " 'betty',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bianca',\n",
       " 'bible',\n",
       " 'bickering',\n",
       " 'bidder',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bill',\n",
       " 'billy',\n",
       " 'bilodeau',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birthday',\n",
       " 'birthdays',\n",
       " 'bit',\n",
       " 'bitches',\n",
       " 'bitter',\n",
       " 'bitterness',\n",
       " 'bivins',\n",
       " 'black',\n",
       " 'blackie',\n",
       " 'blackwell',\n",
       " 'blame',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blesses',\n",
       " 'blessing',\n",
       " 'blessings',\n",
       " 'blind',\n",
       " 'blocked',\n",
       " 'blood',\n",
       " 'blooded',\n",
       " 'blow',\n",
       " 'blue',\n",
       " 'bo',\n",
       " 'boast',\n",
       " 'bob',\n",
       " 'bobby',\n",
       " 'body',\n",
       " 'bohannon',\n",
       " 'bond',\n",
       " 'bones',\n",
       " 'boocub',\n",
       " 'booger',\n",
       " 'book',\n",
       " 'borderline',\n",
       " 'born',\n",
       " 'boss',\n",
       " 'boswell',\n",
       " 'both',\n",
       " 'bothering',\n",
       " 'bottom',\n",
       " 'bounce',\n",
       " 'boy',\n",
       " 'boycott',\n",
       " 'boys',\n",
       " 'brad',\n",
       " 'brag',\n",
       " 'brandon',\n",
       " 'brandy',\n",
       " 'brazzil',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breed',\n",
       " 'brenda',\n",
       " 'brent',\n",
       " 'brian',\n",
       " 'brick',\n",
       " 'bridget',\n",
       " 'brief',\n",
       " 'brien',\n",
       " 'brigitta',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'bro',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'bros',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'bruce',\n",
       " 'bruno',\n",
       " 'brutality',\n",
       " 'bryan',\n",
       " 'bryant',\n",
       " 'bubba',\n",
       " 'bubble',\n",
       " 'buddies',\n",
       " 'bug',\n",
       " 'build',\n",
       " 'building',\n",
       " 'bullet',\n",
       " 'bump',\n",
       " 'bunch',\n",
       " 'bund',\n",
       " 'buntion',\n",
       " 'burden',\n",
       " 'burdge',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'bury',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'buster',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'butterfly',\n",
       " 'by',\n",
       " 'bye',\n",
       " 'byers',\n",
       " 'cab',\n",
       " 'cadena',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'calling',\n",
       " 'calmed',\n",
       " 'calmness',\n",
       " 'calvary',\n",
       " 'came',\n",
       " 'camille',\n",
       " 'campbell',\n",
       " 'campo',\n",
       " 'can',\n",
       " 'cancellation',\n",
       " 'candle',\n",
       " 'canfield',\n",
       " 'cannon',\n",
       " 'cannot',\n",
       " 'cantu',\n",
       " 'capable',\n",
       " 'capital',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'caring',\n",
       " 'carl',\n",
       " 'carla',\n",
       " 'carlos',\n",
       " 'carma',\n",
       " 'carol',\n",
       " 'carolyn',\n",
       " 'carrie',\n",
       " 'carried',\n",
       " 'carrier',\n",
       " 'carrol',\n",
       " 'carry',\n",
       " 'carrying',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'cassandra',\n",
       " 'cast',\n",
       " 'castro',\n",
       " 'catacombs',\n",
       " 'catharina',\n",
       " 'catholic',\n",
       " 'cathy',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causes',\n",
       " 'causing',\n",
       " 'cca',\n",
       " 'cdl',\n",
       " 'cease',\n",
       " 'celebrate',\n",
       " 'celebrating',\n",
       " 'celebration',\n",
       " 'celeste',\n",
       " 'celia',\n",
       " 'celina',\n",
       " 'cell',\n",
       " 'cemetery',\n",
       " 'center',\n",
       " 'central',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chad',\n",
       " 'chains',\n",
       " 'chamber',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'chant',\n",
       " 'chantal',\n",
       " 'chaplain',\n",
       " 'chaplains',\n",
       " 'chapter',\n",
       " 'charge',\n",
       " 'charged',\n",
       " 'chariot',\n",
       " 'charles',\n",
       " 'charlie',\n",
       " 'charlotte',\n",
       " 'charlton',\n",
       " 'cheap',\n",
       " 'cheated',\n",
       " 'check',\n",
       " 'chelsea',\n",
       " 'cheri',\n",
       " 'cherish',\n",
       " 'chester',\n",
       " 'chiara',\n",
       " 'chicken',\n",
       " 'chico',\n",
       " 'chief',\n",
       " 'child',\n",
       " 'childish',\n",
       " 'children',\n",
       " 'chimurenga',\n",
       " 'chiquita',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choke',\n",
       " 'chong',\n",
       " 'chooses',\n",
       " 'choosing',\n",
       " 'chops',\n",
       " 'chose',\n",
       " 'chosen',\n",
       " 'chris',\n",
       " 'christ',\n",
       " 'christi',\n",
       " 'christian',\n",
       " 'christianity',\n",
       " 'christians',\n",
       " 'christina',\n",
       " 'christine',\n",
       " 'christopher',\n",
       " 'chronicle',\n",
       " 'chuckie',\n",
       " 'chump',\n",
       " 'church',\n",
       " 'cigarette',\n",
       " 'cindy',\n",
       " 'circle',\n",
       " 'circled',\n",
       " 'circuit',\n",
       " 'circumstances',\n",
       " 'citizens',\n",
       " 'civilian',\n",
       " 'civilized',\n",
       " 'claimed',\n",
       " 'claims',\n",
       " 'clanging',\n",
       " 'clark',\n",
       " 'clash',\n",
       " 'claudia',\n",
       " 'clay',\n",
       " 'clayton',\n",
       " 'clean',\n",
       " 'cleanse',\n",
       " 'clear',\n",
       " 'clemency',\n",
       " 'cleve',\n",
       " 'client',\n",
       " 'clinton',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closeness',\n",
       " 'closer',\n",
       " 'closet',\n",
       " 'closing',\n",
       " 'closure',\n",
       " 'clothes',\n",
       " 'clown',\n",
       " 'club',\n",
       " 'co',\n",
       " 'coaches',\n",
       " 'code',\n",
       " 'cody',\n",
       " 'cold',\n",
       " 'colleen',\n",
       " 'collier',\n",
       " 'collins',\n",
       " 'color',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comfort',\n",
       " 'comforted',\n",
       " 'coming',\n",
       " 'command',\n",
       " 'commence',\n",
       " 'commend',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'commit',\n",
       " 'committed',\n",
       " 'committing',\n",
       " 'communicate',\n",
       " 'community',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'compassion',\n",
       " 'compelling',\n",
       " 'compensate',\n",
       " 'complaints',\n",
       " 'completely',\n",
       " 'compromise',\n",
       " 'concern',\n",
       " 'conclusion',\n",
       " 'condemned',\n",
       " 'condition',\n",
       " 'conference',\n",
       " 'confess',\n",
       " 'confessed',\n",
       " 'confession',\n",
       " 'confidence',\n",
       " 'confinement',\n",
       " 'conflict',\n",
       " 'confusion',\n",
       " 'conna',\n",
       " 'connie',\n",
       " 'conscience',\n",
       " 'consciousness',\n",
       " 'consequences',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'consolation',\n",
       " 'conspiracy',\n",
       " 'constitution',\n",
       " 'consul',\n",
       " 'consulado',\n",
       " 'consulate',\n",
       " 'contact',\n",
       " 'contacting',\n",
       " 'contaminate',\n",
       " 'contigo',\n",
       " 'continually',\n",
       " 'continue',\n",
       " 'continued',\n",
       " 'contradict',\n",
       " 'contrite',\n",
       " 'control',\n",
       " 'conversion',\n",
       " 'convict',\n",
       " 'convicted',\n",
       " 'conviction',\n",
       " 'cool',\n",
       " 'copy',\n",
       " 'coretta',\n",
       " 'corey',\n",
       " 'cori',\n",
       " 'corinthians',\n",
       " 'corner',\n",
       " 'corpus',\n",
       " 'correct',\n",
       " 'correctional',\n",
       " 'correctly',\n",
       " 'cort',\n",
       " 'cory',\n",
       " 'cost',\n",
       " 'cough',\n",
       " 'coughed',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'counsel',\n",
       " 'counseling',\n",
       " 'count',\n",
       " 'countless',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'counts',\n",
       " 'county',\n",
       " 'couple',\n",
       " 'courage',\n",
       " 'courageous',\n",
       " 'course',\n",
       " 'court',\n",
       " 'courtesy',\n",
       " 'courtroom',\n",
       " 'courts',\n",
       " 'cousins',\n",
       " 'cover',\n",
       " 'cowboy',\n",
       " 'cowboys',\n",
       " 'cox',\n",
       " 'cpr',\n",
       " 'crack',\n",
       " 'craft',\n",
       " 'crain',\n",
       " 'crazy',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'creature',\n",
       " 'cried',\n",
       " 'crime',\n",
       " 'crimes',\n",
       " 'criminal',\n",
       " 'crooked',\n",
       " 'cross',\n",
       " 'crotch',\n",
       " 'crouch',\n",
       " 'crucified',\n",
       " 'cruel',\n",
       " 'crushed',\n",
       " 'cruz',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'cuba',\n",
       " 'cuida',\n",
       " 'cullen',\n",
       " 'cup',\n",
       " 'cut',\n",
       " 'cutler',\n",
       " 'cycle',\n",
       " 'cymbal',\n",
       " 'cynthia',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'daffy',\n",
       " 'daily',\n",
       " 'dallas',\n",
       " 'damen',\n",
       " 'damien',\n",
       " 'damn',\n",
       " 'damned',\n",
       " 'dan',\n",
       " 'dana',\n",
       " 'danalyn',\n",
       " 'dance',\n",
       " 'dancing',\n",
       " 'daniel',\n",
       " 'danielle',\n",
       " 'danny',\n",
       " 'darkness',\n",
       " 'darlie',\n",
       " 'darling',\n",
       " 'darrian',\n",
       " 'daughter',\n",
       " 'daughters',\n",
       " 'david',\n",
       " 'davis',\n",
       " 'dawn',\n",
       " 'dawson',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dea',\n",
       " 'dead',\n",
       " 'deadline',\n",
       " 'deaf',\n",
       " 'deaky',\n",
       " 'deal',\n",
       " 'dealer',\n",
       " 'dealers',\n",
       " 'dealing',\n",
       " 'deals',\n",
       " 'dealt',\n",
       " 'dean',\n",
       " 'dear',\n",
       " 'dearly',\n",
       " 'death',\n",
       " 'debbie',\n",
       " 'debra',\n",
       " 'debt',\n",
       " 'debtors',\n",
       " 'debts',\n",
       " 'deceased',\n",
       " 'deceit',\n",
       " 'deceive',\n",
       " 'december',\n",
       " 'deception',\n",
       " 'decir',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'declare',\n",
       " 'declined',\n",
       " 'dee',\n",
       " 'deeds',\n",
       " 'deemed',\n",
       " 'deena',\n",
       " 'deep',\n",
       " 'deepest',\n",
       " 'deeply',\n",
       " 'defeats',\n",
       " 'defendant',\n",
       " 'defended',\n",
       " 'defense',\n",
       " 'defenseless',\n",
       " 'definitely',\n",
       " 'deidra',\n",
       " 'deleted',\n",
       " 'delia',\n",
       " 'deliberately',\n",
       " 'delight',\n",
       " 'deliver',\n",
       " 'delivered',\n",
       " 'delivers',\n",
       " 'delivery',\n",
       " 'dell',\n",
       " 'demand',\n",
       " 'demonstrate',\n",
       " 'denial',\n",
       " 'denied',\n",
       " 'depart',\n",
       " 'department',\n",
       " 'departure',\n",
       " 'deputy',\n",
       " 'describe',\n",
       " 'deserve',\n",
       " 'deserved',\n",
       " 'desire',\n",
       " 'desired',\n",
       " 'desires',\n",
       " 'desperately',\n",
       " 'despicable',\n",
       " 'despite',\n",
       " 'destiny',\n",
       " 'destroy',\n",
       " 'destroyed',\n",
       " 'destroying',\n",
       " 'destruct',\n",
       " 'deter',\n",
       " 'determine',\n",
       " 'determined',\n",
       " 'detes',\n",
       " 'devastated',\n",
       " 'devastating',\n",
       " 'devil',\n",
       " 'devin',\n",
       " 'devoted',\n",
       " 'devout',\n",
       " 'diamond',\n",
       " 'diamonds',\n",
       " 'diana',\n",
       " 'dianne',\n",
       " 'diaper',\n",
       " 'diarmed',\n",
       " 'diaz',\n",
       " 'dich',\n",
       " 'dickerson',\n",
       " 'did',\n",
       " 'didmau',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'died',\n",
       " 'diego',\n",
       " 'dies',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'digging',\n",
       " 'dignity',\n",
       " 'diligence',\n",
       " 'diminished',\n",
       " 'dios',\n",
       " 'directed',\n",
       " 'direction',\n",
       " 'director',\n",
       " 'dirty',\n",
       " 'disappears',\n",
       " 'disappoint',\n",
       " 'disappointed',\n",
       " 'disappointment',\n",
       " 'disciple',\n",
       " 'disciples',\n",
       " 'discomfort',\n",
       " 'discontinue',\n",
       " 'discovery',\n",
       " 'discriminates',\n",
       " 'discussion',\n",
       " 'dishonor',\n",
       " 'dismiss',\n",
       " 'dispose',\n",
       " 'disrespect',\n",
       " 'district',\n",
       " 'divine',\n",
       " 'dixie',\n",
       " 'dna',\n",
       " 'do',\n",
       " 'doctor',\n",
       " 'doctors',\n",
       " 'document',\n",
       " 'documents',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'doing',\n",
       " 'dolatschko',\n",
       " 'dollars',\n",
       " 'dominican',\n",
       " 'don',\n",
       " 'donahue',\n",
       " 'donating',\n",
       " 'done',\n",
       " 'donna',\n",
       " 'donovan',\n",
       " 'doodle',\n",
       " 'door',\n",
       " 'doors',\n",
       " 'dope',\n",
       " 'doreen',\n",
       " 'doris',\n",
       " 'dorothy',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'doubts',\n",
       " 'doug',\n",
       " 'douglas',\n",
       " 'dow',\n",
       " 'down',\n",
       " 'downtown',\n",
       " 'dozen',\n",
       " 'dr',\n",
       " 'drag',\n",
       " 'drawing',\n",
       " 'drawn',\n",
       " 'dre',\n",
       " 'dreamed',\n",
       " 'dreaming',\n",
       " 'dreams',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'driving',\n",
       " 'drop',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_vocabulary = list(vocabulary)\n",
    "\n",
    "unique_vocabulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
